{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "humanitarian-reputation",
   "metadata": {},
   "source": [
    "## IMDB MOVIE SENTIMENT PREDICATION PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "compound-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "behind-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "gentle-digit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/TARUN KUMAR/Downloads/archive (8)/IMDB.csv\")\n",
    "print(df.head(n=5))\n",
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "amino-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.asarray(df)\n",
    "X_train = df[:,0]\n",
    "Y_train = df[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "intermediate-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoder for prediction values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y_train=le.fit_transform(Y_train)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "surgical-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x1_train,x1_test,y1_train,y1_test=train_test_split(X_train,Y_train,test_size=0.2)\n",
    "x1_train=x1_train[:]\n",
    "y1_train=y1_train[:]\n",
    "x1_test=x1_test[:]\n",
    "y1_test=y1_test[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-certificate",
   "metadata": {},
   "source": [
    "### Cleaning Data :- Stopwords removal, Regex(punctuation) removal, Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "expired-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "tokenier = RegexpTokenizer(r'\\w+')\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "manual-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanReview(review):\n",
    "    review = review.lower()\n",
    "    review = review.replace(\"<br /><br />\",\" \")\n",
    "    tokens = tokenier.tokenize(review)\n",
    "    new_tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    stemmed_tokens = [ps.stem(token) for token in new_tokens]\n",
    "    cleaned_review = ' '.join(stemmed_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "quality-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "x_clean = [getCleanReview(i) for i in x1_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "recognized-circle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-shell",
   "metadata": {},
   "source": [
    "### Create  Vectorize Documents (CountVecotizer and TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "blind-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "flexible-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()\n",
    "x1cv=cv.fit_transform(x1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "advance-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "x1tf=tfidf.fit_transform(x1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-moore",
   "metadata": {},
   "source": [
    "### Train a Classification Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "moderate-performer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# using multinomail naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(x1cv,y1_train)\n",
    "mnb.score(x1cv,y1_train)\n",
    "\n",
    "x1cv_val=cv.transform(x1_val)\n",
    "x1cv_test=cv.transform(x1_test)\n",
    "\n",
    "mnb.score(x1cv_test,y1_test)\n",
    "y_pred=mnb.predict(x1cv_test)\n",
    "print(y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "opened-current",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[868 130]\n",
      " [185 817]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y1_test, y_pred, average='binary')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf=confusion_matrix(y1_test,y_pred)\n",
    "print(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "raising-toilet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix : \n",
      "[[885 113]\n",
      " [167 835]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.880801687763713, 0.8333333333333334, 0.8564102564102564, None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb1=MultinomialNB()\n",
    "mnb1.fit(x1tf,y1_train)\n",
    "mnb1.score(x1tf,y1_train)\n",
    "\n",
    "x1tf_test=tfidf.transform(x1_test)\n",
    "mnb1.score(x1tf_test,y1_test)\n",
    "y_pred1=mnb1.predict(x1tf_test)\n",
    "cnf1=confusion_matrix(y1_test,y_pred1)\n",
    "print(\"confusion matrix : \")\n",
    "print(cnf1)\n",
    "precision_recall_fscore_support(y1_test, y_pred1, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-person",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
